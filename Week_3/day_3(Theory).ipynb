{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a7dcbdb",
   "metadata": {},
   "source": [
    "# üß† First: Why Tokenizers Exist\n",
    "\n",
    "Neural networks **cannot understand text**.\n",
    "\n",
    "They only understand:\n",
    "\n",
    "```\n",
    "Numbers (tensors)\n",
    "```\n",
    "\n",
    "So before a model processes:\n",
    "\n",
    "```\n",
    "\"I love AI\"\n",
    "```\n",
    "\n",
    "It must convert it into:\n",
    "\n",
    "```\n",
    "[1045, 1567, 9932]\n",
    "```\n",
    "\n",
    "That conversion is done by the **tokenizer**.\n",
    "\n",
    "---\n",
    "\n",
    "# üìå Simple Definition\n",
    "\n",
    "> A tokenizer converts raw text into numerical tokens that a transformer model can process.\n",
    "\n",
    "But that‚Äôs surface level.\n",
    "\n",
    "Let‚Äôs go deeper.\n",
    "\n",
    "---\n",
    "\n",
    "# üèó The Full Tokenization Pipeline\n",
    "\n",
    "When you call:\n",
    "\n",
    "```python\n",
    "tokenizer(\"I love AI\")\n",
    "```\n",
    "\n",
    "Internally it does:\n",
    "\n",
    "1. Normalization\n",
    "2. Pre-tokenization\n",
    "3. Subword tokenization\n",
    "4. Convert tokens ‚Üí IDs\n",
    "5. Add special tokens\n",
    "6. Create attention masks\n",
    "\n",
    "Let‚Äôs break these down.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Normalization\n",
    "\n",
    "This step cleans text.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* Lowercasing (for uncased models)\n",
    "* Removing accents\n",
    "* Unicode normalization\n",
    "* Stripping spaces\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "\"Caf√©\"\n",
    "```\n",
    "\n",
    "becomes:\n",
    "\n",
    "```\n",
    "\"cafe\"\n",
    "```\n",
    "\n",
    "(depending on model)\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Pre-Tokenization\n",
    "\n",
    "Splits text into basic word-like pieces.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "\"I love AI!\"\n",
    "```\n",
    "\n",
    "might become:\n",
    "\n",
    "```\n",
    "[\"I\", \"love\", \"AI\", \"!\"]\n",
    "```\n",
    "\n",
    "But this is not final tokenization yet.\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Subword Tokenization (Most Important)\n",
    "\n",
    "This is where modern tokenizers differ from old word tokenizers.\n",
    "\n",
    "Instead of storing entire words,\n",
    "they break words into smaller reusable pieces.\n",
    "\n",
    "Why?\n",
    "\n",
    "Because language is infinite.\n",
    "\n",
    "You can‚Äôt store every possible word.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "Word:\n",
    "\n",
    "```\n",
    "unhappiness\n",
    "```\n",
    "\n",
    "Might become:\n",
    "\n",
    "```\n",
    "[\"un\", \"happi\", \"ness\"]\n",
    "```\n",
    "\n",
    "Or:\n",
    "\n",
    "```\n",
    "[\"un\", \"##happiness\"]\n",
    "```\n",
    "\n",
    "Depending on algorithm.\n",
    "\n",
    "This solves:\n",
    "\n",
    "* Unknown words\n",
    "* Memory explosion\n",
    "* Rare words\n",
    "\n",
    "---\n",
    "\n",
    "# üî¨ Types of Tokenization Algorithms\n",
    "\n",
    "There are several major ones:\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. BPE (Byte Pair Encoding)\n",
    "\n",
    "Used by:\n",
    "\n",
    "* GPT-2\n",
    "* RoBERTa\n",
    "\n",
    "Works by:\n",
    "\n",
    "* Starting with characters\n",
    "* Iteratively merging frequent pairs\n",
    "\n",
    "Example:\n",
    "\n",
    "Start:\n",
    "\n",
    "```\n",
    "u n h a p p i n e s s\n",
    "```\n",
    "\n",
    "Frequent pairs merge:\n",
    "\n",
    "```\n",
    "un happiness\n",
    "```\n",
    "\n",
    "Eventually:\n",
    "\n",
    "```\n",
    "unhappiness\n",
    "```\n",
    "\n",
    "If common enough.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. WordPiece\n",
    "\n",
    "Used by:\n",
    "\n",
    "* BERT\n",
    "\n",
    "Similar to BPE but uses likelihood scoring.\n",
    "\n",
    "Uses prefix markers like:\n",
    "\n",
    "```\n",
    "[\"play\", \"##ing\"]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. SentencePiece\n",
    "\n",
    "Used by:\n",
    "\n",
    "* T5\n",
    "* LLaMA\n",
    "\n",
    "Works directly on raw text (no whitespace splitting).\n",
    "\n",
    "Uses:\n",
    "\n",
    "* Unigram Language Model\n",
    "* BPE variant\n",
    "\n",
    "Handles multilingual text well.\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Convert Tokens ‚Üí IDs\n",
    "\n",
    "After subword splitting:\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "[\"I\", \"love\", \"AI\"]\n",
    "```\n",
    "\n",
    "Each token has a fixed ID in vocabulary:\n",
    "\n",
    "```\n",
    "[1045, 1567, 9932]\n",
    "```\n",
    "\n",
    "The vocabulary is fixed during training.\n",
    "\n",
    "For GPT-2:\n",
    "~50,000 tokens.\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Special Tokens\n",
    "\n",
    "Transformer models need special tokens.\n",
    "\n",
    "Examples:\n",
    "\n",
    "For BERT:\n",
    "\n",
    "```\n",
    "[CLS] I love AI [SEP]\n",
    "```\n",
    "\n",
    "For GPT:\n",
    "\n",
    "```\n",
    "<|startoftext|> I love AI\n",
    "```\n",
    "\n",
    "These tokens tell the model:\n",
    "\n",
    "* Where sequence begins\n",
    "* Where it ends\n",
    "* Where segments separate\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Attention Masks\n",
    "\n",
    "After tokenization, model receives:\n",
    "\n",
    "```\n",
    "input_ids\n",
    "attention_mask\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "input_ids:      [101, 1045, 1567, 9932, 102, 0, 0]\n",
    "attention_mask: [ 1,   1,    1,    1,   1, 0, 0]\n",
    "```\n",
    "\n",
    "Mask tells model:\n",
    "\n",
    "* 1 ‚Üí real token\n",
    "* 0 ‚Üí padding token\n",
    "\n",
    "Without attention mask,\n",
    "model would attend to padding.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Deep Mathematical Understanding\n",
    "\n",
    "Transformer input is:\n",
    "\n",
    "```\n",
    "(batch_size, sequence_length)\n",
    "```\n",
    "\n",
    "Each token ID is mapped to:\n",
    "\n",
    "```\n",
    "Embedding vector (dimension = hidden_size)\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "```\n",
    "input_ids ‚Üí embedding lookup ‚Üí dense vectors\n",
    "```\n",
    "\n",
    "Example:\n",
    "\n",
    "If hidden_size = 768:\n",
    "\n",
    "Each token becomes:\n",
    "\n",
    "```\n",
    "768-dimensional vector\n",
    "```\n",
    "\n",
    "That is what enters self-attention.\n",
    "\n",
    "Tokenizer determines:\n",
    "\n",
    "* Vocabulary size\n",
    "* Token granularity\n",
    "* Sequence length\n",
    "* Memory usage\n",
    "* Model efficiency\n",
    "\n",
    "---\n",
    "\n",
    "# ‚ö† Why Tokenizer Matters So Much\n",
    "\n",
    "Changing tokenizer:\n",
    "\n",
    "* Changes vocabulary\n",
    "* Changes token boundaries\n",
    "* Changes training dynamics\n",
    "* Makes model incompatible\n",
    "\n",
    "You cannot swap tokenizers randomly.\n",
    "\n",
    "Model and tokenizer are tightly coupled.\n",
    "\n",
    "---\n",
    "\n",
    "# üî• Important Engineering Effects\n",
    "\n",
    "### 1Ô∏è‚É£ Token count affects cost\n",
    "\n",
    "In APIs:\n",
    "\n",
    "More tokens ‚Üí More cost\n",
    "\n",
    "Example:\n",
    "\n",
    "\"ChatGPT\" might be:\n",
    "\n",
    "* 1 token\n",
    "* Or 2 tokens depending on tokenizer\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Token length affects memory\n",
    "\n",
    "Attention complexity:\n",
    "\n",
    "```\n",
    "O(n¬≤)\n",
    "```\n",
    "\n",
    "If sequence length doubles,\n",
    "memory usage ~ quadruples.\n",
    "\n",
    "Tokenizer influences sequence length.\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Multilingual Handling\n",
    "\n",
    "SentencePiece handles:\n",
    "\n",
    "* Chinese\n",
    "* Japanese\n",
    "* Hindi\n",
    "  without whitespace assumptions.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Why Subword Tokenization Is Genius\n",
    "\n",
    "It balances:\n",
    "\n",
    "* Word-level meaning\n",
    "* Character-level flexibility\n",
    "\n",
    "Example:\n",
    "\n",
    "New word:\n",
    "\n",
    "```\n",
    "quantumizing\n",
    "```\n",
    "\n",
    "Model never saw it,\n",
    "but tokenizer splits:\n",
    "\n",
    "```\n",
    "[\"quantum\", \"izing\"]\n",
    "```\n",
    "\n",
    "So model can still understand it.\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Quick Mental Model\n",
    "\n",
    "Tokenizer = Language compressor.\n",
    "\n",
    "It compresses infinite language into finite vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "# üì¶ Hugging Face Tokenizer Components\n",
    "\n",
    "In HF:\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "```\n",
    "\n",
    "Internally includes:\n",
    "\n",
    "* vocab.json\n",
    "* merges.txt (for BPE)\n",
    "* tokenizer_config.json\n",
    "* special tokens config\n",
    "\n",
    "---\n",
    "\n",
    "# üî¨ Extremely Deep Insight\n",
    "\n",
    "Tokenization determines:\n",
    "\n",
    "* How model generalizes\n",
    "* How it handles morphology\n",
    "* How it handles unknown words\n",
    "* How long sequences become\n",
    "* How efficient inference is\n",
    "\n",
    "Some research even says:\n",
    "Tokenizer choice affects model intelligence.\n",
    "\n",
    "---\n",
    "\n",
    "# üéì Interview-Level Summary\n",
    "\n",
    "> A tokenizer converts raw text into model-understandable numerical tokens using subword algorithms like BPE, WordPiece, or SentencePiece. It performs normalization, splitting, vocabulary mapping, and padding, producing input IDs and attention masks for transformer models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
