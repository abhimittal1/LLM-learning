{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06f6f253",
   "metadata": {},
   "source": [
    "# Welcome to Pipelines!\n",
    "\n",
    "The HuggingFace transformers library provides APIs at two different levels.\n",
    "\n",
    "The High Level API for using open-source models for typical inference tasks is called \"pipelines\". It's incredibly easy to use.\n",
    "\n",
    "You create a pipeline using something like:\n",
    "\n",
    "`my_pipeline = pipeline(\"the_task_I_want_to_do\")`\n",
    "\n",
    "Followed by\n",
    "\n",
    "`result = my_pipeline(my_input)`\n",
    "\n",
    "And that's it!\n",
    "\n",
    "See end of this colab for a list of all pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1026832",
   "metadata": {},
   "source": [
    "## Before we start: 2 important pro-tips for using Colab:\n",
    "\n",
    "**Pro-tip 1:**\n",
    "\n",
    "Data Science code often gives warnings and messages. They can mostly be safely ignored! Glance over them, and if something goes wrong later, perhaps they can give you a clue.\n",
    "\n",
    "**Pro-tip 2:**\n",
    "\n",
    "In the middle of running a Colab, you might get an error like this:\n",
    "\n",
    "> Runtime error: CUDA is required but not available for bitsandbytes. Please consider installing [...]\n",
    "\n",
    "This is a super-misleading error message! Please don't try changing versions of packages...\n",
    "\n",
    "This actually happens because Google has switched out your Colab runtime, perhaps because Google Colab was too busy. The solution is:\n",
    "\n",
    "1. Kernel menu >> Disconnect and delete runtime\n",
    "2. Reload the colab from fresh and Edit menu >> Clear All Outputs\n",
    "3. Connect to a new T4 using the button at the top right\n",
    "4. Select \"View resources\" from the menu on the top right to confirm you have a GPU\n",
    "5. Rerun the cells in the colab, from the top down, starting with the pip installs\n",
    "\n",
    "And all should work great - otherwise, ask me!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ea9463",
   "metadata": {},
   "source": [
    "## A sidenote:\n",
    "\n",
    "You may already know this, but just in case you're not familiar with the word \"inference\" that I use here:\n",
    "\n",
    "When working with Data Science models, you could be carrying out 2 very different activities: **training** and **inference**.\n",
    "\n",
    "### 1. Training  \n",
    "\n",
    "**Training** is when you provide a model with data for it to adapt to get better at a task in the future. It does this by updating its internal settings - the parameters or weights of the model. If you're Training a model that's already had some training, the activity is called \"fine-tuning\".\n",
    "\n",
    "### 2. Inference\n",
    "\n",
    "**Inference** is when you are working with a model that has _already been trained_. You are using that model to produce new outputs on new inputs, taking advantage of everything it learned while it was being trained. Inference is also sometimes referred to as \"Execution\" or \"Running a model\".\n",
    "\n",
    "All of our use of APIs for GPT, Claude and Gemini in the last weeks are examples of **inference**. The \"P\" in GPT stands for \"Pre-trained\", meaning that it has already been trained with data (lots of it!) In week 6 we will try fine-tuning GPT ourselves.\n",
    "  \n",
    "The pipelines API in HuggingFace is only for use for **inference** - running a model that has already been trained. In week 7 we will be training our own model, and we will need to use the more advanced HuggingFace APIs that we look at in the up-coming lecture.\n",
    "\n",
    "I recorded this playlist on YouTube with more on parameters, training and inference:  \n",
    "https://www.youtube.com/playlist?list=PLWHe-9GP9SMMdl6SLaovUQF2abiLGbMjs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5a7677",
   "metadata": {},
   "source": [
    "# code :\n",
    "\n",
    "https://colab.research.google.com/drive/1OtcXeMpD7JtDTEZ_6qLWJQ6Swi7eLnrn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b86398",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a3dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
