{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "627db934",
   "metadata": {},
   "source": [
    "# ü§ó What Is Hugging Face?\n",
    "\n",
    "**Hugging Face** is an AI company and open platform that provides:\n",
    "\n",
    "* Open-source machine learning models\n",
    "* Tools to train and use models\n",
    "* Infrastructure to host models\n",
    "* APIs for inference\n",
    "* A model-sharing ecosystem\n",
    "\n",
    "Think of it as:\n",
    "\n",
    "> üß† GitHub + NPM + DockerHub ‚Äî but for AI models.\n",
    "\n",
    "---\n",
    "\n",
    "# üèó What Hugging Face Actually Provides\n",
    "\n",
    "There are 4 major components:\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ ü§ñ Model Hub (Most Famous Part)\n",
    "\n",
    "This is a massive public repository of ML models.\n",
    "\n",
    "It contains:\n",
    "\n",
    "* LLMs (Llama, Mistral, Falcon, etc.)\n",
    "* Vision models\n",
    "* Speech models\n",
    "* Embedding models\n",
    "* Diffusion models\n",
    "* Fine-tuned community models\n",
    "\n",
    "You can:\n",
    "\n",
    "* Download models\n",
    "* Fine-tune them\n",
    "* Deploy them\n",
    "* Share your own models\n",
    "\n",
    "Think of it like:\n",
    "\n",
    "> PyPI for neural networks\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ üß© Transformers Library\n",
    "\n",
    "This is a Python library to use models easily.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "print(generator(\"Hello world\"))\n",
    "```\n",
    "\n",
    "Behind the scenes:\n",
    "\n",
    "* Loads model\n",
    "* Loads tokenizer\n",
    "* Handles preprocessing\n",
    "* Runs inference\n",
    "\n",
    "Before Hugging Face, this was very hard.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ üöÄ Inference API\n",
    "\n",
    "You don‚Äôt want to download 7GB models?\n",
    "\n",
    "You can call their hosted API.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
    "headers = {\"Authorization\": \"Bearer YOUR_TOKEN\"}\n",
    "\n",
    "response = requests.post(API_URL, headers=headers, json={\"inputs\": \"Hello\"})\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "This is similar to OpenAI API ‚Äî but for open models.\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ üñ• Spaces (Like AI Apps Hosting)\n",
    "\n",
    "Hugging Face Spaces lets you:\n",
    "\n",
    "* Deploy ML apps\n",
    "* Use Gradio or Streamlit\n",
    "* Share demos publicly\n",
    "\n",
    "Many AI demos you see online are hosted on HF Spaces.\n",
    "\n",
    "---\n",
    "\n",
    "# üß† How It Differs From OpenAI\n",
    "\n",
    "| OpenAI            | Hugging Face       |\n",
    "| ----------------- | ------------------ |\n",
    "| Closed models     | Mostly open models |\n",
    "| API-first         | Model-first        |\n",
    "| Hosted service    | Open ecosystem     |\n",
    "| Controlled access | Community-driven   |\n",
    "\n",
    "OpenAI ‚Üí You call their models\n",
    "Hugging Face ‚Üí You can host, modify, fine-tune\n",
    "\n",
    "---\n",
    "\n",
    "# üî¨ Why Hugging Face Is Important for You\n",
    "\n",
    "Since you're:\n",
    "\n",
    "* Learning LLM systems\n",
    "* Interested in AI agents\n",
    "* Building pipelines\n",
    "\n",
    "Hugging Face allows you to:\n",
    "\n",
    "‚úî Run local LLMs\n",
    "‚úî Fine-tune models\n",
    "‚úî Use embedding models for RAG\n",
    "‚úî Experiment without API cost\n",
    "‚úî Benchmark models\n",
    "\n",
    "---\n",
    "\n",
    "# üß© Common Use Cases\n",
    "\n",
    "### 1Ô∏è‚É£ Embeddings for RAG\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Text Classification\n",
    "\n",
    "```python\n",
    "pipeline(\"sentiment-analysis\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Fine-Tuning\n",
    "\n",
    "You can fine-tune models on your dataset.\n",
    "\n",
    "This is huge for:\n",
    "\n",
    "* Domain-specific chatbots\n",
    "* Legal AI\n",
    "* Medical AI\n",
    "* Custom resume parsers\n",
    "\n",
    "---\n",
    "\n",
    "# üè¢ Why It Became So Big\n",
    "\n",
    "Because it:\n",
    "\n",
    "* Standardized model loading\n",
    "* Made model sharing easy\n",
    "* Built a strong open community\n",
    "* Simplified research ‚Üí production pipeline\n",
    "\n",
    "---\n",
    "\n",
    "# ‚öôÔ∏è Architecture-Level View\n",
    "\n",
    "When you use Hugging Face locally:\n",
    "\n",
    "User ‚Üí Python ‚Üí Transformers ‚Üí Model Weights ‚Üí GPU ‚Üí Output\n",
    "\n",
    "When you use API:\n",
    "\n",
    "User ‚Üí HF Server ‚Üí Hosted Model ‚Üí Output\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Advanced Understanding\n",
    "\n",
    "Hugging Face is not just a company.\n",
    "\n",
    "It‚Äôs an ecosystem:\n",
    "\n",
    "* Datasets library\n",
    "* Evaluate library\n",
    "* Accelerate library\n",
    "* PEFT (Parameter Efficient Fine Tuning)\n",
    "* Diffusers (for image generation)\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ In One Clean Definition\n",
    "\n",
    "Hugging Face is:\n",
    "\n",
    "> The open-source infrastructure layer of modern AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25489fc8",
   "metadata": {},
   "source": [
    "# üî∑ Core Hugging Face Libraries\n",
    "\n",
    "These are the main ones you must know.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ `transformers` (Most Important)\n",
    "\n",
    "### What it does:\n",
    "\n",
    "Loads and runs pretrained models for:\n",
    "\n",
    "* LLMs (text generation)\n",
    "* Classification\n",
    "* Question answering\n",
    "* Translation\n",
    "* Summarization\n",
    "* Vision models\n",
    "* Speech models\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "generator(\"Hello world\")\n",
    "```\n",
    "\n",
    "### Internally handles:\n",
    "\n",
    "* Tokenization\n",
    "* Model loading\n",
    "* Preprocessing\n",
    "* Postprocessing\n",
    "\n",
    "This is the backbone of HF.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ `datasets`\n",
    "\n",
    "### What it does:\n",
    "\n",
    "Loads and manages datasets efficiently.\n",
    "\n",
    "It supports:\n",
    "\n",
    "* Large datasets (streaming)\n",
    "* Versioning\n",
    "* Efficient memory usage\n",
    "* Preprocessing\n",
    "\n",
    "### Example:\n",
    "\n",
    "```python\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "print(dataset[\"train\"][0])\n",
    "```\n",
    "\n",
    "Used heavily in:\n",
    "\n",
    "* Fine-tuning\n",
    "* Evaluation\n",
    "* Benchmarking\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ `tokenizers`\n",
    "\n",
    "### What it does:\n",
    "\n",
    "Fast tokenization library written in Rust.\n",
    "\n",
    "LLMs don‚Äôt read text ‚Äî they read tokens.\n",
    "\n",
    "This library:\n",
    "\n",
    "* Converts text ‚Üí tokens\n",
    "* Handles subword tokenization (BPE, WordPiece, etc.)\n",
    "* Extremely fast\n",
    "\n",
    "Used internally by `transformers`.\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ `accelerate`\n",
    "\n",
    "### What it does:\n",
    "\n",
    "Helps run models efficiently on:\n",
    "\n",
    "* Multiple GPUs\n",
    "* Mixed precision\n",
    "* Distributed training\n",
    "* TPU\n",
    "\n",
    "It abstracts hardware complexity.\n",
    "\n",
    "Without it ‚Üí distributed training is painful.\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ `evaluate`\n",
    "\n",
    "### What it does:\n",
    "\n",
    "Standardized evaluation metrics.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Accuracy\n",
    "* BLEU\n",
    "* ROUGE\n",
    "* F1\n",
    "* Perplexity\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "accuracy.compute(predictions=[0,1], references=[0,1])\n",
    "```\n",
    "\n",
    "Used in:\n",
    "\n",
    "* Model benchmarking\n",
    "* Research evaluation\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ `peft` (Parameter-Efficient Fine-Tuning)\n",
    "\n",
    "Very important in modern AI.\n",
    "\n",
    "PEFT allows:\n",
    "\n",
    "* LoRA\n",
    "* Adapters\n",
    "* Prefix tuning\n",
    "\n",
    "Instead of fine-tuning entire 7B model,\n",
    "you fine-tune small parameters.\n",
    "\n",
    "Huge cost savings.\n",
    "\n",
    "Used in:\n",
    "\n",
    "* Custom LLM training\n",
    "* Domain adaptation\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ `diffusers`\n",
    "\n",
    "For image generation models.\n",
    "\n",
    "Supports:\n",
    "\n",
    "* Stable Diffusion\n",
    "* Text-to-image\n",
    "* Image-to-image\n",
    "* Inpainting\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from diffusers import StableDiffusionPipeline\n",
    "```\n",
    "\n",
    "Used in generative AI apps.\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ `trl` (Transformer Reinforcement Learning)\n",
    "\n",
    "Used for:\n",
    "\n",
    "* RLHF\n",
    "* PPO training\n",
    "* Preference optimization\n",
    "\n",
    "Advanced alignment training.\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ `optimum`\n",
    "\n",
    "Optimizes models for:\n",
    "\n",
    "* ONNX\n",
    "* TensorRT\n",
    "* Intel\n",
    "* AWS Inferentia\n",
    "\n",
    "For production deployment.\n",
    "\n",
    "---\n",
    "\n",
    "# üî∑ Infrastructure & Platform Libraries\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ `huggingface_hub`\n",
    "\n",
    "Allows you to:\n",
    "\n",
    "* Upload models\n",
    "* Download models\n",
    "* Manage repositories\n",
    "* Access HF Hub programmatically\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "from huggingface_hub import hf_hub_download\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ `gradio`\n",
    "\n",
    "Technically separate but tightly integrated.\n",
    "\n",
    "Used for:\n",
    "\n",
    "* Building AI demos\n",
    "* Deploying interfaces on Spaces\n",
    "\n",
    "You already mentioned Gradio earlier üòâ\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Ecosystem Structure (Big Picture)\n",
    "\n",
    "You can think of Hugging Face stack like this:\n",
    "\n",
    "```\n",
    "User Code\n",
    "   ‚Üì\n",
    "transformers\n",
    "   ‚Üì\n",
    "tokenizers\n",
    "   ‚Üì\n",
    "accelerate (hardware optimization)\n",
    "   ‚Üì\n",
    "Model weights (from huggingface_hub)\n",
    "```\n",
    "\n",
    "For training:\n",
    "\n",
    "```\n",
    "datasets + transformers + accelerate + peft\n",
    "```\n",
    "\n",
    "For evaluation:\n",
    "\n",
    "```\n",
    "evaluate\n",
    "```\n",
    "\n",
    "For image models:\n",
    "\n",
    "```\n",
    "diffusers\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üî• If You're Becoming an AI Engineer\n",
    "\n",
    "The most important for you:\n",
    "\n",
    "1. transformers\n",
    "2. datasets\n",
    "3. huggingface_hub\n",
    "4. peft\n",
    "5. accelerate\n",
    "\n",
    "Those 5 are core.\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Simple Summary\n",
    "\n",
    "| Library         | Purpose               |\n",
    "| --------------- | --------------------- |\n",
    "| transformers    | Run models            |\n",
    "| datasets        | Load data             |\n",
    "| tokenizers      | Convert text ‚Üí tokens |\n",
    "| accelerate      | Scale training        |\n",
    "| evaluate        | Measure performance   |\n",
    "| peft            | Efficient fine-tuning |\n",
    "| diffusers       | Image generation      |\n",
    "| huggingface_hub | Access model hub      |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec2d557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
