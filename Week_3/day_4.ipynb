{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53079ec1",
   "metadata": {},
   "source": [
    "# 1ï¸âƒ£ Simple Intuition: What is Quantization?\n",
    "\n",
    "Imagine you have:\n",
    "\n",
    "* A number like:\n",
    "  **3.1415926535**\n",
    "\n",
    "But you decide to store it as:\n",
    "\n",
    "* **3.14**\n",
    "\n",
    "You reduced precision.\n",
    "\n",
    "Thatâ€™s basically **quantization**.\n",
    "\n",
    "---\n",
    "\n",
    "### In LLM terms:\n",
    "\n",
    "Large Language Models store:\n",
    "\n",
    "* Billions of numbers (weights)\n",
    "* Usually in **32-bit floating point (FP32)** format\n",
    "\n",
    "Quantization means:\n",
    "\n",
    "> Converting these high-precision numbers into lower precision numbers\n",
    "> like:\n",
    "\n",
    "* FP16 (16-bit)\n",
    "* INT8 (8-bit)\n",
    "* INT4 (4-bit)\n",
    "\n",
    "So instead of storing:\n",
    "\n",
    "```\n",
    "0.123456789\n",
    "```\n",
    "\n",
    "We approximate it to:\n",
    "\n",
    "```\n",
    "0.12\n",
    "```\n",
    "\n",
    "We lose a little precision,\n",
    "but save a LOT of memory.\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£ Why Do We Need Quantization?\n",
    "\n",
    "Letâ€™s take an example.\n",
    "\n",
    "Suppose:\n",
    "\n",
    "* A model has 7 billion parameters\n",
    "* Each parameter stored in FP32 (4 bytes)\n",
    "\n",
    "Memory required:\n",
    "\n",
    "```\n",
    "7B Ã— 4 bytes = 28 GB\n",
    "```\n",
    "\n",
    "That means:\n",
    "\n",
    "* You need 28GB VRAM to load it!\n",
    "\n",
    "But if we use INT8 (1 byte per parameter):\n",
    "\n",
    "```\n",
    "7B Ã— 1 byte = 7 GB\n",
    "```\n",
    "\n",
    "ðŸ”¥ Now it fits on a normal GPU.\n",
    "\n",
    "---\n",
    "\n",
    "### So quantization helps in:\n",
    "\n",
    "* âœ… Reducing memory\n",
    "* âœ… Faster inference\n",
    "* âœ… Lower power usage\n",
    "* âœ… Running LLMs on laptops / edge devices\n",
    "* âœ… Cheaper deployment\n",
    "\n",
    "---\n",
    "\n",
    "# 3ï¸âƒ£ What Actually Happens Mathematically?\n",
    "\n",
    "Originally weights are like:\n",
    "\n",
    "```\n",
    "W = 0.245789\n",
    "```\n",
    "\n",
    "In FP32 â†’ very precise.\n",
    "\n",
    "In INT8 â†’ we only have 256 possible values:\n",
    "\n",
    "```\n",
    "-128 to 127\n",
    "```\n",
    "\n",
    "So we:\n",
    "\n",
    "1. Find the range of values (min, max)\n",
    "2. Scale them into 256 buckets\n",
    "3. Store bucket index instead of full float\n",
    "\n",
    "### Formula idea (simplified):\n",
    "\n",
    "We use:\n",
    "\n",
    "[\n",
    "q = round(\\frac{w}{scale})\n",
    "]\n",
    "\n",
    "Where:\n",
    "\n",
    "* w = original weight\n",
    "* scale = compression factor\n",
    "* q = quantized integer\n",
    "\n",
    "During inference we do:\n",
    "\n",
    "[\n",
    "w â‰ˆ q Ã— scale\n",
    "]\n",
    "\n",
    "So it's compressed but approximately reconstructed.\n",
    "\n",
    "---\n",
    "\n",
    "# 4ï¸âƒ£ Types of Quantization\n",
    "\n",
    "There are multiple types ðŸ‘‡\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1. Post-Training Quantization (PTQ)\n",
    "\n",
    "* Train model normally (FP32)\n",
    "* Quantize AFTER training\n",
    "\n",
    "Fast and simple.\n",
    "\n",
    "Used in:\n",
    "\n",
    "* GGUF models\n",
    "* GPTQ\n",
    "* bitsandbytes\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2. Quantization-Aware Training (QAT)\n",
    "\n",
    "* During training, simulate quantization\n",
    "* Model learns to adapt to low precision\n",
    "\n",
    "More accurate\n",
    "But expensive to train\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3. Dynamic vs Static Quantization\n",
    "\n",
    "### Dynamic\n",
    "\n",
    "* Quantize weights only\n",
    "* Activations stay float\n",
    "* Done during inference\n",
    "\n",
    "### Static\n",
    "\n",
    "* Quantize both weights and activations\n",
    "* Faster\n",
    "* Requires calibration dataset\n",
    "\n",
    "---\n",
    "\n",
    "# 5ï¸âƒ£ Different Precision Levels\n",
    "\n",
    "Hereâ€™s what they mean:\n",
    "\n",
    "| Type | Bits | Memory     | Accuracy        | Common Use   |\n",
    "| ---- | ---- | ---------- | --------------- | ------------ |\n",
    "| FP32 | 32   | Very High  | Best            | Training     |\n",
    "| FP16 | 16   | Half       | Almost same     | Inference    |\n",
    "| INT8 | 8    | 4x smaller | Slight drop     | Production   |\n",
    "| INT4 | 4    | 8x smaller | Noticeable drop | Edge devices |\n",
    "| INT2 | 2    | Very tiny  | Risky           | Research     |\n",
    "\n",
    "---\n",
    "\n",
    "# 6ï¸âƒ£ Real-World LLM Quantization Methods\n",
    "\n",
    "In LLM ecosystem youâ€™ll hear:\n",
    "\n",
    "### ðŸ”¹ GPTQ\n",
    "\n",
    "Quantization optimized for transformers.\n",
    "\n",
    "### ðŸ”¹ AWQ (Activation-aware Weight Quantization)\n",
    "\n",
    "Improves accuracy by looking at activations.\n",
    "\n",
    "### ðŸ”¹ bitsandbytes (by Tim Dettmers)\n",
    "\n",
    "Very popular for 8-bit + 4-bit quantization.\n",
    "\n",
    "### ðŸ”¹ GGUF (used in llama.cpp)\n",
    "\n",
    "Used to run LLMs locally.\n",
    "\n",
    "---\n",
    "\n",
    "# 7ï¸âƒ£ Where Does Quantization Happen?\n",
    "\n",
    "In transformer layers:\n",
    "\n",
    "* Linear layers (most memory heavy)\n",
    "* Attention layers\n",
    "* MLP layers\n",
    "\n",
    "These have large weight matrices like:\n",
    "\n",
    "```\n",
    "[4096 Ã— 4096]\n",
    "```\n",
    "\n",
    "Quantization compresses these matrices.\n",
    "\n",
    "---\n",
    "\n",
    "# 8ï¸âƒ£ The Trade-Off\n",
    "\n",
    "Important concept:\n",
    "\n",
    "### More compression â†’ More error\n",
    "\n",
    "Because:\n",
    "\n",
    "We approximate numbers.\n",
    "\n",
    "But surprisingly:\n",
    "\n",
    "ðŸ‘‰ LLMs are very robust\n",
    "They tolerate small numeric errors very well.\n",
    "\n",
    "Thatâ€™s why INT8 works almost as good as FP16.\n",
    "\n",
    "---\n",
    "\n",
    "# 9ï¸âƒ£ Why LLMs Can Tolerate Quantization?\n",
    "\n",
    "Because:\n",
    "\n",
    "* They are overparameterized\n",
    "* Many weights are redundant\n",
    "* Neural networks are noise tolerant\n",
    "\n",
    "So small rounding errors donâ€™t break logic.\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£0ï¸âƒ£ Analogy\n",
    "\n",
    "Think of it like:\n",
    "\n",
    "ðŸ“¸ Original photo â†’ 4K resolution\n",
    "ðŸ“± Compressed photo â†’ 1080p\n",
    "\n",
    "Still looks good to human eye.\n",
    "\n",
    "Quantization = compressing LLM without destroying intelligence.\n",
    "\n",
    "---\n",
    "\n",
    "# ðŸ”¥ Final Summary (One Line)\n",
    "\n",
    "> Quantization is the process of converting high-precision model weights into lower-precision numbers to reduce memory usage and speed up inference while keeping performance almost the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733ed8fc",
   "metadata": {},
   "source": [
    "CODE : https://colab.research.google.com/drive/1hhR9Z-yiqjUe7pJjVQw4c74z_V3VchLy?usp=sharing"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
